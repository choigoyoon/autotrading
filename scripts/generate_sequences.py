import json
import shutil
import warnings
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, cast, TypedDict

import joblib  # type: ignore[reportMissingTypeStubs]
import numpy as np
import numpy.typing as npt
import pandas as pd
import torch
from pandas import DataFrame, Index, Series
from sklearn.preprocessing import MinMaxScaler  # type: ignore[reportMissingTypeStubs]
from tqdm import tqdm

warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)

class SequenceConfig(TypedDict):
    sequence_length: int
    test_size: float
    val_size: float
    labels_dir: Path
    ohlcv_path: Path
    output_dir: Path
    feature_cols: List[str]

class SequenceGenerator:
    """
    ë¼ë²¨ë§ëœ ì‹œê³„ì—´ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµìš© ì‹œí€€ìŠ¤ë¥¼ ìƒì„±, ì •ê·œí™”, ë¶„í•  ë° ì €ì¥í•©ë‹ˆë‹¤.
    """
    def __init__(self, config: SequenceConfig):
        self.config = config
        self.scaler = MinMaxScaler()

    def run(self) -> bool:
        """ì „ì²´ ì‹œí€€ìŠ¤ ìƒì„± íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        print("ğŸš€ MACD ë¼ë²¨ ê¸°ë°˜ ì‹œí€€ìŠ¤ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # 1. ë°ì´í„° ë¡œë“œ
        df_features, df_labels, label_column = self._load_and_prepare_data()
        if df_features is None or df_labels is None or label_column is None:
            return False

        # 2. ë°ì´í„° ì •ê·œí™” ë° ë°°ì—´ ë³€í™˜
        feature_array, label_array = self._scale_and_convert_to_arrays(df_features, df_labels, label_column)

        # 3. ë°ì´í„°ì…‹ ë¶„í•  ë° ì €ì¥
        self._split_and_save_sequences(feature_array, label_array, df_features.index)
        
        # 4. ë©”íƒ€ë°ì´í„° ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        self._save_metadata_and_scaler(df_features, df_labels, label_column)
        
        print(f"\nâœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: {self.config['output_dir']}")
        return True

    def _load_and_prepare_data(self) -> Tuple[Optional[DataFrame], Optional[DataFrame], Optional[str]]:
        """ë¼ë²¨ê³¼ OHLCV ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        try:
            # ë¼ë²¨ íŒŒì¼ ì„ íƒ
            label_file = self._find_label_file()
            if not label_file: return None, None, None
            print(f"ğŸ¯ ì‚¬ìš©í•  ë¼ë²¨ íŒŒì¼: {label_file.name}")

            df_labels = pd.read_parquet(label_file)
            df_ohlcv = pd.read_parquet(self.config['ohlcv_path'])

            # ì¸ë±ìŠ¤ ë° ì»¬ëŸ¼ ì •ë¦¬
            df_labels, label_column = self._clean_labels(df_labels)
            if label_column is None: return None, None, None
            
            df_ohlcv = self._clean_ohlcv(df_ohlcv)

            # í”¼ì²˜ ìƒì„±
            df_features = self._create_features(df_ohlcv)

            # ê³µí†µ ì¸ë±ìŠ¤ë¡œ ë°ì´í„° ì •ë ¬
            common_idx = df_features.index.intersection(df_labels.index)
            if len(common_idx) < self.config['sequence_length'] * 2:
                print("âŒ ê³µí†µ ì¸ë±ìŠ¤ì˜ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.")
                return None, None, None
            
            return df_features.loc[common_idx], df_labels.loc[common_idx], label_column

        except FileNotFoundError as e:
            print(f"âŒ íŒŒì¼ ì—†ìŒ: {e}")
            return None, None, None
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return None, None, None

    def _find_label_file(self) -> Optional[Path]:
        """ì‚¬ìš©í•  ìµœì ì˜ ë¼ë²¨ íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤."""
        if not self.config['labels_dir'].exists():
            raise FileNotFoundError(f"ë¼ë²¨ ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.config['labels_dir']}")
        
        label_files = list(self.config['labels_dir'].glob('*.parquet'))
        if not label_files:
            print(f"âŒ ë¼ë²¨ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.config['labels_dir']}")
            return None
            
        # ìš°ì„ ìˆœìœ„: merged > 1min > ê°€ì¥ í° íŒŒì¼
        merged_file = next((f for f in label_files if 'merged' in f.name.lower()), None)
        if merged_file: return merged_file

        one_min_file = next((f for f in label_files if '1min' in f.name.lower()), None)
        if one_min_file: return one_min_file

        return max(label_files, key=lambda f: f.stat().st_size)

    def _clean_labels(self, df: DataFrame) -> Tuple[DataFrame, Optional[str]]:
        """ë¼ë²¨ ë°ì´í„°í”„ë ˆì„ì˜ ì¸ë±ìŠ¤ì™€ ì»¬ëŸ¼ì„ ì •ë¦¬í•©ë‹ˆë‹¤."""
        if 'timestamp' in df.columns:
            df = df.set_index('timestamp')
        if not isinstance(df.index, pd.DatetimeIndex):
            df.index = pd.to_datetime(df.index)
        df.index = df.index.tz_localize(None) # Timezone ì œê±°

        label_col = next((c for c in ['label', 'labels', 'target'] if c in df.columns), None)
        if not label_col:
            print("âŒ 'label', 'labels' ë˜ëŠ” 'target' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return df, None
        return df, label_col

    def _clean_ohlcv(self, df: DataFrame) -> DataFrame:
        """OHLCV ë°ì´í„°í”„ë ˆì„ì˜ ì¸ë±ìŠ¤ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
        if 'timestamp' in df.columns:
            df = df.set_index('timestamp')
        if not isinstance(df.index, pd.DatetimeIndex):
            df.index = pd.to_datetime(df.index)
        df.index = df.index.tz_localize(None) # Timezone ì œê±°
        return df
        
    def _create_features(self, df_ohlcv: DataFrame) -> DataFrame:
        """í”¼ì²˜ë¥¼ ìƒì„±í•˜ê³  ì„ íƒí•©ë‹ˆë‹¤."""
        df = df_ohlcv.copy()
        df['return'] = df['close'].pct_change().fillna(0)
        df['volatility'] = df['return'].rolling(20).std().fillna(0)
        df['volume_ma'] = df['volume'].rolling(20).mean().fillna(df['volume'])
        
        available_features = [f for f in self.config['feature_cols'] if f in df.columns]
        return df.loc[:, available_features]

    def _scale_and_convert_to_arrays(
        self, 
        df_features: DataFrame, 
        df_labels: DataFrame, 
        label_column: str
    ) -> Tuple[npt.NDArray[np.float32], npt.NDArray[np.int_]]:
        """ë°ì´í„°ë¥¼ ì •ê·œí™”í•˜ê³  NumPy ë°°ì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        feature_array: npt.NDArray[np.float32] = self.scaler.fit_transform(df_features).astype(np.float32)
        label_array: npt.NDArray[np.int_] = df_labels[label_column].to_numpy().astype(np.int_)
        return feature_array, label_array

    def _split_and_save_sequences(
        self, 
        feature_array: npt.NDArray[np.float32], 
        label_array: npt.NDArray[np.int_], 
        index: Index
    ) -> None:
        """ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ê³  train/val/testë¡œ ë¶„í• í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤."""
        output_base_dir = self.config['output_dir']
        if output_base_dir.exists():
            shutil.rmtree(output_base_dir)
        
        num_sequences = len(feature_array) - self.config['sequence_length'] + 1
        if num_sequences <= 0:
            print("âŒ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ê¸°ì— ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return

        train_end = int(num_sequences * (1 - self.config['test_size'] - self.config['val_size']))
        val_end = int(num_sequences * (1 - self.config['test_size']))
        
        datasets = {"train": (0, train_end), "val": (train_end, val_end), "test": (val_end, num_sequences)}

        for name, (start, end) in datasets.items():
            dataset_dir = output_base_dir / name
            dataset_dir.mkdir(parents=True)
            for i in tqdm(range(start, end), desc=f"'{name}' ë°ì´í„° ì €ì¥ ì¤‘"):
                seq_end_idx = i + self.config['sequence_length'] - 1
                sequence = feature_array[i : seq_end_idx + 1]
                label = label_array[seq_end_idx]
                timestamp = cast(pd.Timestamp, index[seq_end_idx])

                torch.save({
                    'features': torch.tensor(sequence, dtype=torch.float32),
                    'label': torch.tensor(label, dtype=torch.long),
                    'timestamp': timestamp
                }, dataset_dir / f"{i - start}.pt")

    def _save_metadata_and_scaler(self, df_features: DataFrame, df_labels: DataFrame, label_column: str) -> None:
        """ë©”íƒ€ë°ì´í„°ì™€ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        metadata = {
            'sequence_length': self.config['sequence_length'],
            'num_features': len(df_features.columns),
            'feature_names': list(df_features.columns),
            'label_column': label_column,
            'data_range': {'start': str(df_features.index[0]), 'end': str(df_features.index[-1])},
            'label_distribution': {str(k): v for k, v in df_labels[label_column].value_counts().to_dict().items()}
        }
        with open(self.config['output_dir'] / "metadata.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        
        joblib.dump(self.scaler, self.config['output_dir'] / "scaler.joblib")


def main() -> bool:
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    project_root = Path(__file__).resolve().parent.parent
    
    config: SequenceConfig = {
        "sequence_length": 240,
        "test_size": 0.1,
        "val_size": 0.2,
        "labels_dir": project_root / 'data' / 'processed' / 'btc_usdt_kst' / 'labeled',
        "ohlcv_path": project_root / 'data' / 'processed' / 'btc_usdt_kst' / 'resampled_ohlcv' / '1min.parquet',
        "output_dir": project_root / 'data' / 'sequences_macd',
        "feature_cols": ['open', 'high', 'low', 'close', 'volume', 'return', 'volatility', 'volume_ma']
    }
    
    generator = SequenceGenerator(config)
    return generator.run()

if __name__ == '__main__':
    if main():
        print("\nğŸ‰ ì‹œí€€ìŠ¤ ìƒì„± ì„±ê³µ!")
    else:
        print("\nâŒ ì‹œí€€ìŠ¤ ìƒì„± ì‹¤íŒ¨!")