import pandas as pd
import numpy as np
from pathlib import Path
import joblib  # type: ignore[import-untyped]
from sklearn.preprocessing import MinMaxScaler
from tqdm import tqdm
from typing import cast
from numpy.typing import NDArray

def create_sequences(
    data: NDArray[np.float64], 
    feature_cols_indices: list[int], 
    label_cols_indices: list[int], 
    sequence_length: int = 60
) -> tuple[NDArray[np.float64], NDArray[np.float64]]:
    """
    ë°ì´í„°í”„ë ˆì„ì—ì„œ ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    sequences: list[NDArray[np.float64]] = []
    labels: list[NDArray[np.float64]] = []
    
    for i in tqdm(range(len(data) - sequence_length), desc="ì‹œí€€ìŠ¤ ìƒì„± ì¤‘"):
        seq: NDArray[np.float64] = data[i:i+sequence_length][:, feature_cols_indices]
        
        # numpy ìŠ¬ë¼ì´ì‹± ê²°ê³¼ëŠ” ìŠ¤ì¹¼ë¼ ë˜ëŠ” ë°°ì—´ì¼ ìˆ˜ ìˆìŒ
        label_raw: float | NDArray[np.float64] = data[i+sequence_length-1][label_cols_indices]
        
        if isinstance(label_raw, np.ndarray):
            label: NDArray[np.float64] = label_raw.astype(np.float64)
        else:
            label = np.array([label_raw], dtype=np.float64)
        
        sequences.append(seq)
        labels.append(label)
    
    sequences_array = np.array(sequences, dtype=np.float64)
    labels_array = np.array(labels, dtype=np.float64)
    
    return sequences_array, labels_array

def validate_data_shapes(X: NDArray[np.float64], y: NDArray[np.float64]) -> tuple[NDArray[np.float64], NDArray[np.float64]]:
    """LSTM í˜¸í™˜ì„±ì„ ìœ„í•œ ë°ì´í„° shape ê²€ì¦ ë° ìˆ˜ì •"""
    if y.ndim == 1:
        y = y.reshape(-1, 1)
        print(f"âš ï¸  y shapeì„ LSTM í˜¸í™˜ í˜•íƒœë¡œ ë³€í™˜: {y.shape}")
    
    assert X.shape[0] == y.shape[0], f"ìƒ˜í”Œ ìˆ˜ ë¶ˆì¼ì¹˜: X={X.shape[0]}, y={y.shape[0]}"
    assert X.ndim == 3, f"XëŠ” 3ì°¨ì› ë°°ì—´ì´ì–´ì•¼ í•¨: í˜„ì¬ {X.ndim}ì°¨ì›"
    assert y.ndim == 2, f"yëŠ” 2ì°¨ì› ë°°ì—´ì´ì–´ì•¼ í•¨: í˜„ì¬ {y.ndim}ì°¨ì›"
    
    print(f"âœ… ë°ì´í„° shape ê²€ì¦ ì™„ë£Œ: X={X.shape}, y={y.shape}")
    return X, y

def main() -> None:
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì‹œì‘ ğŸš€")
    
    features_path = Path('results/ml_analysis_v2/enhanced_features_dataset.parquet')
    labels_path = Path('data/processed/btc_usdt_kst/advanced_labels.parquet')
    output_dir = Path('results/sequences')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("ğŸ“‚ ë°ì´í„° ë¡œë”©...")
    features_df: pd.DataFrame = pd.read_parquet(features_path)
    labels_df: pd.DataFrame = pd.read_parquet(labels_path)
    
    # íƒ€ì„ì¡´ í†µì¼
    if not isinstance(features_df.index, pd.DatetimeIndex):
        features_df.index = pd.to_datetime(features_df.index, utc=True)
    features_df.index = features_df.index.tz_convert('UTC')
    
    if not isinstance(labels_df.index, pd.DatetimeIndex):
        labels_df.index = pd.to_datetime(labels_df.index, utc=True)
    labels_df.index = labels_df.index.tz_convert('UTC')

    print("ğŸ”„ ë°ì´í„° ë³‘í•© ì¤‘...")
    merged_data: pd.DataFrame = pd.merge(
        features_df, 
        labels_df[['trade_type']], 
        left_index=True, 
        right_index=True, 
        how='inner'
    )
    
    data_with_dummies = pd.get_dummies(merged_data, columns=['type', 'timeframe', 'trade_type'], drop_first=True)
    data = cast(pd.DataFrame, data_with_dummies)

    feature_cols: list[str] = [col for col in data.columns if 'trade_type' not in col]
    label_cols: list[str] = [col for col in data.columns if 'trade_type' in col]

    print(f"ğŸ“Š í”¼ì²˜ ì»¬ëŸ¼ ìˆ˜: {len(feature_cols)}")
    print(f"ğŸ“Š ë¼ë²¨ ì»¬ëŸ¼ ìˆ˜: {len(label_cols)}")

    print("ğŸ“ ë°ì´í„° ì •ê·œí™” ì¤‘...")
    scaler = MinMaxScaler()
    
    # .values ëŠ” NDArray[Any]ë¥¼ ë°˜í™˜í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ëª…ì‹œì ìœ¼ë¡œ float64ë¡œ ë³€í™˜
    scaler_input: NDArray[np.float64] = data[feature_cols].values.astype(np.float64)
    scaled_features_result = scaler.fit_transform(scaler_input)
    # fit_transform ê²°ê³¼ëŠ” NDArrayì´ë¯€ë¡œ cast
    scaled_features = cast(NDArray[np.float64], scaled_features_result)

    _ = joblib.dump(scaler, output_dir / 'sequence_scaler.joblib')
    
    data_scaled = data.copy()
    data_scaled[feature_cols] = scaled_features
    
    feature_cols_indices: list[int] = [cast(int, data.columns.get_loc(c)) for c in feature_cols]
    label_cols_indices: list[int] = [cast(int, data.columns.get_loc(c)) for c in label_cols]

    sequence_length: int = 60
    
    # .values ëŠ” NDArray[Any]ë¥¼ ë°˜í™˜í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ëª…ì‹œì ìœ¼ë¡œ float64ë¡œ ë³€í™˜
    data_values: NDArray[np.float64] = data_scaled.values.astype(np.float64)
    
    X, y = create_sequences(
        data_values, 
        feature_cols_indices, 
        label_cols_indices,
        sequence_length
    )
    
    x_validated, y_validated = validate_data_shapes(X, y)
    
    print(f"ğŸ“Š ìµœì¢… ì‹œí€€ìŠ¤ Shape:")
    print(f"   X (í”¼ì²˜): {x_validated.shape} - (ìƒ˜í”Œìˆ˜, ì‹œí€€ìŠ¤ê¸¸ì´, í”¼ì²˜ìˆ˜)")
    print(f"   y (ë¼ë²¨): {y_validated.shape} - (ìƒ˜í”Œìˆ˜, ë¼ë²¨ìˆ˜)")
    print(f"   X dtype: {x_validated.dtype}")
    print(f"   y dtype: {y_validated.dtype}")
    
    metadata = {
        'feature_cols': feature_cols,
        'label_cols': label_cols,
        'sequence_length': sequence_length,
        'scaler_features': getattr(scaler, 'n_features_in_', 0),
        'data_shape': x_validated.shape,
        'label_shape': y_validated.shape
    }
    
    print("ğŸ’¾ ì‹œí€€ìŠ¤ ë°ì´í„° ì €ì¥ ì¤‘...")
    np.save(output_dir / 'sequences_X.npy', x_validated)
    np.save(output_dir / 'sequences_y.npy', y_validated)
    _ = joblib.dump(metadata, output_dir / 'metadata.joblib')
    
    print(f"âœ… ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: {output_dir}")
    print(f"ğŸ“‹ ì €ì¥ëœ íŒŒì¼:")
    print(f"   - sequences_X.npy: í”¼ì²˜ ì‹œí€€ìŠ¤ ë°ì´í„°")
    print(f"   - sequences_y.npy: ë¼ë²¨ ë°ì´í„°")
    print(f"   - metadata.joblib: ë©”íƒ€ë°ì´í„° (í”¼ì²˜ ì»¬ëŸ¼ í¬í•¨)")
    print(f"   - sequence_scaler.joblib: ì •ê·œí™” ìŠ¤ì¼€ì¼ëŸ¬")

if __name__ == "__main__":
    main() 