"""
íŒŒì¼ëª…: project_sync_report.py
ìš°ì£¼ì•„ë¹ ë‹˜ + Claude + Cursor AI 3ì ë™ê¸°í™”ìš© ë¦¬í¬íŠ¸
"""
import sys
import platform
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

import psutil  # type: ignore
import pandas as pd
from loguru import logger

# --- Constants ---
IMPORTANT_PATHS: List[str] = [
    ".", "src", "src/analysis", "src/data", "src/features", "src/labeling",
    "data", "data/processed", "data/raw", "results", "models", "tools", "scripts", "configs"
]

KEY_SCRIPTS: List[str] = [
    'tools/pipeline_runner.py',
    'src/analysis/label_validation_analyzer.py',
    'src/analysis/timeframe_consensus_analyzer.py',
    'src/analysis/divergence_quantifier.py'
]

# --- Helper Functions ---
def get_directory_info(path: Path) -> Dict[str, Any]:
    """ì£¼ì–´ì§„ ê²½ë¡œì˜ íŒŒì¼ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not path.is_dir():
        return {}
    
    all_files = list(path.iterdir())
    return {
        'python_files': [f.name for f in all_files if f.name.endswith('.py')],
        'data_files': [f.name for f in all_files if f.name.endswith(('.parquet', '.csv'))],
        'config_files': [f.name for f in all_files if f.name.endswith(('.txt', '.md', '.toml', '.py', '.yaml', '.json'))],
        'total_files': len(all_files)
    }

def get_project_root() -> Path:
    """í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ src/utilsì— ìˆë‹¤ê³  ê°€ì •
    return Path(__file__).resolve().parent.parent.parent

# --- Main Report Generation ---
def generate_sync_report() -> None:
    """í”„ë¡œì íŠ¸ ë™ê¸°í™” ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ê³  í„°ë¯¸ë„ì— ì¶œë ¥í•˜ë©°, JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    project_root = get_project_root()
    logger.info(f"Project root directory: {project_root}")
    
    print("ğŸ¯ í€€íŠ¸ë§¤ë§¤ í”„ë¡œì íŠ¸ 3ì ë™ê¸°í™” ë¦¬í¬íŠ¸")
    print("=" * 80)
    
    # 1. ì»´í“¨í„° ì‚¬ì–‘ ì •ë³´
    print("\nğŸ’» ì»´í“¨í„° ì‚¬ì–‘:")
    print(f"OS: {platform.system()} {platform.release()}")
    try:
        print(f"CPU: {platform.processor()}")
    except Exception:
        print("CPU: ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print(f"CPU ì½”ì–´: {psutil.cpu_count(logical=True)}ê°œ (Physical: {psutil.cpu_count(logical=False)})")
    print(f"ë©”ëª¨ë¦¬: {round(psutil.virtual_memory().total / (1024**3), 1)} GB")
    print(f"Python: {sys.version.split()[0]}")
    
    # 2. í”„ë¡œì íŠ¸ í´ë” êµ¬ì¡°
    print("\nğŸ“ í”„ë¡œì íŠ¸ í´ë” êµ¬ì¡°:")
    project_map: Dict[str, Dict[str, Any]] = {}
    for path_str in IMPORTANT_PATHS:
        path = project_root / path_str
        if path.exists():
            dir_info = get_directory_info(path)
            project_map[path_str] = dir_info
            total_files = dir_info.get('total_files', 0)
            print(f"  ğŸ“‚ {path_str:<25}: {total_files}ê°œ íŒŒì¼")
    
    # 3. í˜„ì¬ ì§„í–‰ ë‹¨ê³„
    print("\nğŸ¯ í˜„ì¬ ì§„í–‰ ë‹¨ê³„:")
    progress: Dict[str, str] = {}
    
    def check_path_content(path_str: str, glob_pattern: str) -> bool:
        path = project_root / path_str
        return path.exists() and any(path.glob(glob_pattern))

    progress['Phase1_ë°ì´í„°ìˆ˜ì§‘'] = "âœ… ì™„ë£Œ" if check_path_content('data/raw', '*.parquet') else "âŒ ë¯¸ì™„ë£Œ"
    progress['Phase1_ë°ì´í„°ê°€ê³µ'] = "âœ… ì™„ë£Œ" if check_path_content('data/processed', '*_features.parquet') else "âŒ ë¯¸ì™„ë£Œ"
    progress['Phase2_ë¼ë²¨ë§'] = "âœ… ì™„ë£Œ" if check_path_content('data/processed', '*_labeled.parquet') else "âŒ ë¯¸ì™„ë£Œ"
    progress['Phase3_ëª¨ë¸í•™ìŠµ'] = "âœ… ì™„ë£Œ" if check_path_content('models', '*.keras') or check_path_content('models', '*.pkl') else "âŒ ë¯¸ì™„ë£Œ"
    
    for phase, status in progress.items():
        print(f"  {phase:<25}: {status}")
    
    # 4. ë°ì´í„° í˜„í™© ìƒì„¸
    print("\nğŸ“Š ë°ì´í„° í˜„í™©:")
    processed_path = project_root / 'data/processed'
    if processed_path.exists():
        labeled_files = list(processed_path.glob('*_labeled.parquet'))
        print(f"  ì²˜ë¦¬ëœ ë¼ë²¨ë§ íŒŒì¼: {len(labeled_files)}ê°œ")
        
        if labeled_files:
            sample_file = labeled_files[0]
            try:
                df = pd.read_parquet(sample_file)
                print(f"  ìƒ˜í”Œ íŒŒì¼: {sample_file.name}")
                if df.index.is_monotonic_increasing:
                    print(f"  ë°ì´í„° ê¸°ê°„: {df.index.min()} ~ {df.index.max()}")
                else:
                    print("  ë°ì´í„° ê¸°ê°„: ì¸ë±ìŠ¤ê°€ ì •ë ¬ë˜ì§€ ì•ŠìŒ")
                print(f"  ë°ì´í„° í¬ê¸°: {len(df)} í–‰, {len(df.columns)} ì—´")
                
                if 'macd_label' in df.columns:
                    label_dist = df['macd_label'].value_counts()
                    print(f"  ë¼ë²¨ ë¶„í¬: ë§¤ìˆ˜({label_dist.get(1,0)}), ë§¤ë„({label_dist.get(-1,0)}), ê´€ë§({label_dist.get(0,0)})")
                    
            except Exception as e:
                logger.error(f"ë°ì´í„° ì½ê¸° ì˜¤ë¥˜: {e}")
                print(f"  ë°ì´í„° ì½ê¸° ì˜¤ë¥˜: {e}")
    
    # 5. ë¶„ì„ ê²°ê³¼ í˜„í™©
    print("\nğŸ“ˆ ë¶„ì„ ê²°ê³¼ í˜„í™©:")
    results_path = project_root / 'results'
    if results_path.exists():
        result_files = [f.name for f in results_path.glob('*.csv')]
        print(f"  ë¶„ì„ ê²°ê³¼ íŒŒì¼: {len(result_files)}ê°œ")
        for file_name in result_files:
            print(f"    - {file_name}")
    else:
        print("  ë¶„ì„ ê²°ê³¼ ì—†ìŒ")
    
    # 6. ì‹¤í–‰ ê°€ëŠ¥í•œ ì£¼ìš” ìŠ¤í¬ë¦½íŠ¸
    print("\nğŸ ì‹¤í–‰ ê°€ëŠ¥í•œ ì£¼ìš” ìŠ¤í¬ë¦½íŠ¸:")
    for script_path_str in KEY_SCRIPTS:
        script_path = project_root / script_path_str
        status = "âœ…" if script_path.exists() else "âŒ"
        print(f"  {status} {script_path_str}")
    
    # 7. ë‹¤ìŒ í•  ì¼ (ê°„ì†Œí™”ëœ ì œì•ˆ)
    print("\nğŸš€ ë‹¤ìŒ ì¶”ì²œ ì‘ì—…:")
    next_step = "ì•Œ ìˆ˜ ì—†ìŒ"
    if progress.get('Phase1_ë°ì´í„°ê°€ê³µ') == 'âŒ ë¯¸ì™„ë£Œ':
        next_step = "1. ë°ì´í„° ì „ì²˜ë¦¬ ë° í”¼ì²˜ ìƒì„± ì‹¤í–‰"
    elif progress.get('Phase2_ë¼ë²¨ë§') == 'âŒ ë¯¸ì™„ë£Œ':
        next_step = "2. ë°ì´í„° ë¼ë²¨ë§ ì‹¤í–‰"
    elif progress.get('Phase3_ëª¨ë¸í•™ìŠµ') == 'âŒ ë¯¸ì™„ë£Œ':
        next_step = "3. ëª¨ë¸ í•™ìŠµ ì‹¤í–‰"
    else:
        next_step = "4. ë°±í…ŒìŠ¤íŒ… ë° ê°€ìƒë§¤ë§¤ ì‹œìŠ¤í…œ ì‹¤í–‰"
    print(f"  â¡ï¸  {next_step}")
    
    # 8. JSON ë¦¬í¬íŠ¸ ì €ì¥
    report_data: Dict[str, Any] = {
        'timestamp': datetime.now().isoformat(),
        'computer_specs': {
            'os': f"{platform.system()} {platform.release()}",
            'cpu_cores': psutil.cpu_count(logical=True),
            'memory_gb': round(psutil.virtual_memory().total / (1024**3), 1),
            'python_version': sys.version.split()[0]
        },
        'project_structure': project_map,
        'progress': progress,
        'next_steps': next_step
    }
    
    report_file = project_root / 'project_sync_report.json'
    try:
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
    except IOError as e:
        logger.error(f"ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        print(f"\nâŒ ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {report_file}")

    print("\n" + "=" * 80)
    print("ğŸ“‹ ì´ ë¦¬í¬íŠ¸ë¥¼ Claudeì™€ Cursor AIì—ê²Œ ê³µìœ í•˜ì„¸ìš”!")

if __name__ == "__main__":
    # ë¡œê·¸ ì„¤ì •
    log_dir = get_project_root() / "logs"
    log_dir.mkdir(exist_ok=True)
    
    logger.remove()
    logger.add(sys.stderr, level="INFO")
    logger.add(log_dir / "sync_report_{time}.log", level="DEBUG", rotation="10 MB")
    
    generate_sync_report()