# type: ignore
# pylint: disable-all
"""
íŒŒì¼ëª…: project_sync_report.py
ìš°ì£¼ì•„ë¹ ë‹˜ + Claude + Cursor AI 3ì ë™ê¸°í™”ìš© ë¦¬í¬íŠ¸
"""

import os
import sys
import psutil
import platform
import pandas as pd
from pathlib import Path
import json
from datetime import datetime

def generate_sync_report():
    print("ğŸ¯ í€€íŠ¸ë§¤ë§¤ í”„ë¡œì íŠ¸ 3ì ë™ê¸°í™” ë¦¬í¬íŠ¸")
    print("=" * 80)
    
    # 1. ì»´í“¨í„° ì‚¬ì–‘ ì •ë³´
    print("\nğŸ’» ì»´í“¨í„° ì‚¬ì–‘:")
    print(f"OS: {platform.system()} {platform.release()}")
    print(f"CPU: {platform.processor()}")
    print(f"CPU ì½”ì–´: {psutil.cpu_count()} ê°œ")
    print(f"ë©”ëª¨ë¦¬: {round(psutil.virtual_memory().total / (1024**3), 1)} GB")
    print(f"Python: {sys.version.split()[0]}")
    
    # 2. í”„ë¡œì íŠ¸ í´ë” êµ¬ì¡°
    print("\nğŸ“ í”„ë¡œì íŠ¸ í´ë” êµ¬ì¡°:")
    project_map = {}
    
    important_paths = [
        ".",
        "src",
        "src/analysis", 
        "src/data",
        "src/indicators",
        "src/labeling",
        "data",
        "data/processed",
        "data/raw", 
        "analysis_results",
        "models",
        "notebooks"
    ]
    
    for path in important_paths:
        if os.path.exists(path):
            files = []
            if os.path.isdir(path):
                all_files = os.listdir(path)
                py_files = [f for f in all_files if f.endswith('.py')]
                data_files = [f for f in all_files if f.endswith(('.parquet', '.csv'))]
                config_files = [f for f in all_files if f.endswith(('.txt', '.md', '.toml', '.py'))]
                
                files = {
                    'python_files': py_files,
                    'data_files': data_files, 
                    'config_files': config_files,
                    'total_files': len(all_files)
                }
            
            project_map[path] = files
            # filesê°€ ë”•ì…”ë„ˆë¦¬ì¼ ë•Œë§Œ .get()ì„ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
            total_files = files.get('total_files', 0) if isinstance(files, dict) else len(files)
            print(f"  ğŸ“‚ {path}: {total_files}ê°œ íŒŒì¼")
    
    # 3. í˜„ì¬ ì§„í–‰ ë‹¨ê³„
    print("\nğŸ¯ í˜„ì¬ ì§„í–‰ ë‹¨ê³„:")
    progress = {}
    
    # Phase 1: ë°ì´í„° ìˆ˜ì§‘ ë° ë¼ë²¨ë§
    has_raw_data = len(list(Path('data/raw').glob('*.parquet'))) > 0 if os.path.exists('data/raw') else False
    has_processed_data = len(list(Path('data/processed').glob('*_indicators.parquet'))) > 0 if os.path.exists('data/processed') else False
    
    progress['Phase1_ë°ì´í„°ìˆ˜ì§‘'] = "âœ… ì™„ë£Œ" if has_raw_data else "âŒ ë¯¸ì™„ë£Œ"
    progress['Phase1_ë¼ë²¨ë§'] = "âœ… ì™„ë£Œ" if has_processed_data else "âŒ ë¯¸ì™„ë£Œ"
    
    # Phase 2: ë¼ë²¨ë§ ê²€ì¦
    has_validation = os.path.exists('analysis_results/label_validation.csv')
    has_consensus = os.path.exists('analysis_results/timeframe_consensus_analysis.csv')
    
    progress['Phase2_ë¼ë²¨ê²€ì¦'] = "âœ… ì™„ë£Œ" if has_validation else "âŒ ë¯¸ì™„ë£Œ"
    progress['Phase2_íƒ€ì„í”„ë ˆì„í•©ì˜'] = "âœ… ì™„ë£Œ" if has_consensus else "âŒ ë¯¸ì™„ë£Œ"
    
    # Phase 3: ë”¥ëŸ¬ë‹ ë¶„ì„  
    has_pattern_analysis = os.path.exists('analysis_results/divergence_strength_analysis.csv')
    has_wm_analysis = os.path.exists('analysis_results/zigzag_pattern_analysis.csv')
    
    progress['Phase3_íŒ¨í„´ë¶„ì„'] = "âœ… ì™„ë£Œ" if has_pattern_analysis else "âŒ ë¯¸ì™„ë£Œ"
    progress['Phase3_WMíŒ¨í„´'] = "âœ… ì™„ë£Œ" if has_wm_analysis else "âŒ ë¯¸ì™„ë£Œ"
    
    for phase, status in progress.items():
        print(f"  {phase}: {status}")
    
    # 4. ë°ì´í„° í˜„í™© ìƒì„¸
    print("\nğŸ“Š ë°ì´í„° í˜„í™©:")
    if os.path.exists('data/processed'):
        parquet_files = list(Path('data/processed').glob('*.parquet'))
        print(f"  ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼: {len(parquet_files)}ê°œ")
        
        if parquet_files:
            sample_file = parquet_files[0]
            try:
                df = pd.read_parquet(sample_file)
                print(f"  ìƒ˜í”Œ íŒŒì¼: {sample_file.name}")
                print(f"  ë°ì´í„° ê¸°ê°„: {len(df)}ê°œ ìº”ë“¤")
                print(f"  ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ")
                print(f"  ë¼ë²¨ë§ ì—¬ë¶€: {'macd_label' in df.columns}")
                
                if 'macd_label' in df.columns:
                    label_dist = df['macd_label'].value_counts()
                    print(f"  ë¼ë²¨ ë¶„í¬: ë§¤ìˆ˜{label_dist.get(1,0)}ê°œ, ë§¤ë„{label_dist.get(-1,0)}ê°œ, ê´€ë§{label_dist.get(0,0)}ê°œ")
                    
            except Exception as e:
                print(f"  ë°ì´í„° ì½ê¸° ì˜¤ë¥˜: {e}")
    
    # 5. ë¶„ì„ ê²°ê³¼ í˜„í™©
    print("\nğŸ“ˆ ë¶„ì„ ê²°ê³¼ í˜„í™©:")
    if os.path.exists('analysis_results'):
        result_files = list(Path('analysis_results').glob('*.csv'))
        print(f"  ë¶„ì„ ê²°ê³¼ íŒŒì¼: {len(result_files)}ê°œ")
        for file in result_files:
            print(f"    - {file.name}")
    else:
        print("  ë¶„ì„ ê²°ê³¼ ì—†ìŒ")
    
    # 6. ì‹¤í–‰ ê°€ëŠ¥í•œ ìŠ¤í¬ë¦½íŠ¸
    print("\nğŸ ì‹¤í–‰ ê°€ëŠ¥í•œ ìŠ¤í¬ë¦½íŠ¸:")
    key_scripts = [
        'pipeline_runner.py',
        'src/analysis/label_validation_analyzer.py',
        'src/analysis/timeframe_consensus_analyzer.py', 
        'src/analysis/divergence_quantifier.py'
    ]
    
    for script in key_scripts:
        if os.path.exists(script):
            print(f"  âœ… {script}")
        else:
            print(f"  âŒ {script}")
    
    # 7. ë‹¤ìŒ í•  ì¼
    print("\nğŸ¯ ë‹¤ìŒ í•  ì¼:")
    
    if not has_processed_data:
        print("  1. pipeline_runner.py ì‹¤í–‰í•´ì„œ ë¼ë²¨ë§ ì™„ë£Œ")
    elif not has_validation:
        print("  1. ë¼ë²¨ë§ ê²€ì¦ ë¶„ì„ ì‹¤í–‰")
    elif not has_consensus:
        print("  1. íƒ€ì„í”„ë ˆì„ í•©ì˜ ë¶„ì„ ì‹¤í–‰")
    elif not has_pattern_analysis:
        print("  1. ë‹¤ì´ë²„ì „ìŠ¤ íŒ¨í„´ ë¶„ì„ ì‹¤í–‰")
    else:
        print("  1. W/M íŒ¨í„´ ì§€ê·¸ì¬ê·¸ ë¶„ì„ ì‹¤í–‰")
        print("  2. ë”¥ëŸ¬ë‹ ì„±ê³µ/ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„")
        print("  3. ë§¤ë§¤ë²• 5ë‹¨ê³„ ê²€ì¦ ì‹œìŠ¤í…œ êµ¬ì¶•")
    
    # 8. JSON ë¦¬í¬íŠ¸ ì €ì¥
    report_data = {
        'timestamp': datetime.now().isoformat(),
        'computer_specs': {
            'os': f"{platform.system()} {platform.release()}",
            'cpu_cores': psutil.cpu_count(),
            'memory_gb': round(psutil.virtual_memory().total / (1024**3), 1),
            'python_version': sys.version.split()[0]
        },
        'project_structure': project_map,
        'progress': progress,
        'next_steps': "ìœ„ ì¶œë ¥ ì°¸ì¡°"
    }
    
    with open('project_sync_report.json', 'w', encoding='utf-8') as f:
        json.dump(report_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥: project_sync_report.json")
    print("\n" + "=" * 80)
    print("ğŸ“‹ ì´ ë¦¬í¬íŠ¸ë¥¼ Claudeì™€ Cursor AIì—ê²Œ ê³µìœ í•˜ì„¸ìš”!")

if __name__ == "__main__":
    generate_sync_report() 