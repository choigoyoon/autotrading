import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed
import numba
import os

@numba.njit(fastmath=True)
def analyze_single_signal(signal_ts, signal_price, signal_label, 
                         ohlcv_timestamps, ohlcv_high, ohlcv_low, ohlcv_close):
    """ë‹¨ì¼ ì‹ í˜¸ ë¶„ì„ (numba ìµœì í™”)"""
    # ì‹ í˜¸ ì´í›„ ë°ì´í„°ë§Œ ì‚¬ìš©
    start_idx = np.searchsorted(ohlcv_timestamps, signal_ts)
    
    if start_idx >= len(ohlcv_high):
        return 0.0, -1, 0, -1  # ìˆ˜ìµë¥ , ìµœê³ ì idx, ìƒíƒœ, ì²­ì‚°idx
    
    future_high = ohlcv_high[start_idx:]
    future_low = ohlcv_low[start_idx:]
    future_close = ohlcv_close[start_idx:]
    
    if len(future_high) == 0:
        return 0.0, -1, 0, -1
    
    # ìµœëŒ€ ìˆ˜ìµë¥  ê³„ì‚°
    max_profit_pct = 0.0
    peak_idx = -1
    
    if signal_label == 1:  # ë§¤ìˆ˜
        peak_idx = np.argmax(future_high)
        peak_price = future_high[peak_idx]
        max_profit_pct = ((peak_price - signal_price) / signal_price) * 100
    else:  # ë§¤ë„
        peak_idx = np.argmin(future_low)
        trough_price = future_low[peak_idx]
        max_profit_pct = ((signal_price - trough_price) / signal_price) * 100
    
    # ë³¸ì ˆ ë³µê·€ ì²´í¬ (ìµœê³ ì  ì´í›„)
    status = 1  # 1: Holding, 2: Breakeven Exit
    exit_idx = -1
    
    if peak_idx < len(future_high) - 1:
        after_peak_high = future_high[peak_idx + 1:]
        after_peak_low = future_low[peak_idx + 1:]
        
        if signal_label == 1:  # ë§¤ìˆ˜ â†’ ì €ì ì´ ì§„ì…ê°€ ì´í•˜ë¡œ
            for i in range(len(after_peak_low)):
                if after_peak_low[i] <= signal_price:
                    status = 2
                    exit_idx = peak_idx + 1 + i
                    break
        else:  # ë§¤ë„ â†’ ê³ ì ì´ ì§„ì…ê°€ ì´ìƒìœ¼ë¡œ
            for i in range(len(after_peak_high)):
                if after_peak_high[i] >= signal_price:
                    status = 2
                    exit_idx = peak_idx + 1 + i
                    break
    
    return max_profit_pct, start_idx + peak_idx, status, start_idx + exit_idx if exit_idx != -1 else -1

def process_timeframe_chunk(args):
    """íƒ€ì„í”„ë ˆì„ ì²­í¬ ì²˜ë¦¬"""
    timeframe, chunk_df, ohlcv_data = args
    
    try:
        # ë°ì´í„° ì¤€ë¹„
        signal_timestamps = chunk_df.index.view('int64')
        signal_prices = chunk_df['close'].values.astype(np.float64)
        signal_labels = chunk_df['label'].values.astype(np.int64)
        
        ohlcv_timestamps = ohlcv_data['timestamps']
        ohlcv_high = ohlcv_data['high']
        ohlcv_low = ohlcv_data['low']
        ohlcv_close = ohlcv_data['close']
        
        results = []
        
        # ê° ì‹ í˜¸ ë¶„ì„
        for i in range(len(signal_timestamps)):
            signal_ts = signal_timestamps[i]
            signal_price = signal_prices[i]
            signal_label = signal_labels[i]
            
            max_profit, peak_idx, status, exit_idx = analyze_single_signal(
                signal_ts, signal_price, signal_label,
                ohlcv_timestamps, ohlcv_high, ohlcv_low, ohlcv_close
            )
            
            # ê²°ê³¼ ì €ì¥
            peak_timestamp = ohlcv_timestamps[peak_idx] if peak_idx != -1 else None
            exit_timestamp = ohlcv_timestamps[exit_idx] if exit_idx != -1 else None
            candles_to_exit = exit_idx - np.searchsorted(ohlcv_timestamps, signal_ts) if exit_idx != -1 else np.nan
            
            results.append({
                'ì§„ì…ì‹œì ': pd.to_datetime(signal_ts),
                'íƒ€ì„í”„ë ˆì„': timeframe,
                'ì§„ì…ê°€ê²©': signal_price,
                'ì‹ í˜¸íƒ€ì…': 'Buy' if signal_label == 1 else 'Sell',
                'ìµœëŒ€ìˆ˜ìµë¥ (%)': max_profit,
                'ìµœê³ (ì €)ì ì‹œì ': pd.to_datetime(peak_timestamp) if peak_timestamp else None,
                'ìµœì¢…ìƒíƒœ': 'Holding' if status == 1 else 'Breakeven Exit',
                'ì²­ì‚°ì‹œì ': pd.to_datetime(exit_timestamp) if exit_timestamp else None,
                'ì²­ì‚°ê¹Œì§€ìº”ë“¤ìˆ˜': candles_to_exit,
            })
        
        return pd.DataFrame(results)
        
    except Exception as e:
        print(f"[ì˜¤ë¥˜] {timeframe}: {e}")
        return None

def analyze_performance():
    """ë³‘ë ¬í™”ëœ ì„±ê³¼ ë¶„ì„"""
    # ğŸ”¥ ê²½ë¡œ ìˆ˜ì •
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    labels_dir = PROJECT_ROOT / 'data' / 'labels_macd' / 'btc_usdt_kst'
    ohlcv_dir = PROJECT_ROOT / 'data' / 'processed' / 'btc_usdt_kst' / 'resampled_ohlcv'
    results_dir = PROJECT_ROOT / 'results' / 'label_performance'
    results_dir.mkdir(parents=True, exist_ok=True)

    merged_labels_file = labels_dir / 'btc_usdt_kst_merged_labels.parquet'
    output_file = results_dir / 'performance_analysis_parallel.csv'
    
    print(f"CPU ì½”ì–´ ìˆ˜: {os.cpu_count()}")
    
    # ë°ì´í„° ë¡œë“œ
    if not merged_labels_file.exists():
        print(f"âŒ ì˜¤ë¥˜: ë³‘í•©ëœ ë¼ë²¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("1. ë³‘í•©ëœ ë¼ë²¨ ë°ì´í„° ë¡œë“œ ì¤‘...")
    signals_df = pd.read_parquet(merged_labels_file)
    print(f"   ì´ {len(signals_df):,}ê°œì˜ ì‹ í˜¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")

    # OHLCV ë¡œë“œ
    print("2. OHLCV ë°ì´í„° ë¡œë“œ ì¤‘...")
    timeframes = signals_df['timeframe'].unique()
    
    # ğŸ”¥ íŒŒì¼ëª… ë§¤í•‘ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜)
    TF_MAP = {
        '1D': '1day', '2D': '2day', '3D': '3day', '1W': '1week',
        '1h': '1h', '2h': '2h', '4h': '4h', '6h': '6h', '8h': '8h', '12h': '12h',
        '1min': '1min', '3min': '3min', '5min': '5min', '10min': '10min', 
        '15min': '15min', '30min': '30min'
    }
    
    ohlcv_data = {}
    for tf in timeframes:
        file_name = TF_MAP.get(tf, tf)
        ohlcv_file = ohlcv_dir / f"{file_name}.parquet"
        
        # íŒŒì¼ëª… ë³€í˜• ì‹œë„
        if not ohlcv_file.exists():
            ohlcv_file = ohlcv_dir / f"{tf.replace('min', 'm')}.parquet"
        
        if ohlcv_file.exists():
            try:
                ohlcv_df = pd.read_parquet(ohlcv_file)
                ohlcv_data[tf] = {
                    'timestamps': ohlcv_df.index.view('int64'),
                    'high': ohlcv_df['high'].values.astype(np.float64),
                    'low': ohlcv_df['low'].values.astype(np.float64),
                    'close': ohlcv_df['close'].values.astype(np.float64)
                }
                print(f"   {tf}: {ohlcv_df.shape}")
            except Exception as e:
                print(f"âš ï¸ {tf} ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            print(f"âš ï¸ {tf} íŒŒì¼ ì—†ìŒ")

    # ğŸ”¥ ë³‘ë ¬ ì²˜ë¦¬
    print("3. ë³‘ë ¬ ì„±ê³¼ ë¶„ì„ ì‹œì‘...")
    
    # íƒ€ì„í”„ë ˆì„ë³„ ì²­í¬ ìƒì„±
    chunks = []
    for tf, group in signals_df.groupby('timeframe'):
        if tf in ohlcv_data:
            # ì²­í¬ í¬ê¸° ì¡°ì • (í° íƒ€ì„í”„ë ˆì„ì€ ì‘ê²Œ ë‚˜ëˆ„ê¸°)
            chunk_size = 50000 if len(group) > 100000 else len(group)
            
            for i in range(0, len(group), chunk_size):
                chunk = group.iloc[i:i+chunk_size]
                chunks.append((tf, chunk, ohlcv_data[tf]))
    
    print(f"   ì´ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
    
    # ë³‘ë ¬ ì‹¤í–‰
    results = []
    max_workers = min(os.cpu_count(), 16)  # ìµœëŒ€ 16ê°œ ì›Œì»¤
    
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_timeframe_chunk, chunk) for chunk in chunks]
        
        for future in tqdm(as_completed(futures), total=len(futures), desc="ë³‘ë ¬ ë¶„ì„"):
            try:
                result = future.result()
                if result is not None and not result.empty:
                    results.append(result)
            except Exception as e:
                print(f"[ì—ëŸ¬] ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

    # ê²°ê³¼ ì •ë¦¬
    print("4. ê²°ê³¼ ì •ë¦¬...")
    if results:
        results_df = pd.concat(results, ignore_index=True)
        results_df.to_csv(output_file, index=False, encoding='utf-8-sig')

        print(f"\nğŸ‰ ì„±ê³¼ ë¶„ì„ ì™„ë£Œ!")
        print(f"âœ… ê²°ê³¼ ì €ì¥: {output_file}")
        print(f"   - ì´ ë¶„ì„ëœ ì‹ í˜¸: {len(results_df):,}ê°œ")
        print(f"   - í‰ê·  ìµœëŒ€ ìˆ˜ìµë¥ : {results_df['ìµœëŒ€ìˆ˜ìµë¥ (%)'].mean():.2f}%")
        
        # ğŸ”¥ ê°„ë‹¨ í†µê³„
        print(f"\nğŸ“Š ë¹ ë¥¸ í†µê³„:")
        print(f"   - Holding: {(results_df['ìµœì¢…ìƒíƒœ'] == 'Holding').sum():,}ê°œ")
        print(f"   - Breakeven Exit: {(results_df['ìµœì¢…ìƒíƒœ'] == 'Breakeven Exit').sum():,}ê°œ")
        print(f"   - ì–‘ìˆ˜ ìˆ˜ìµë¥ : {(results_df['ìµœëŒ€ìˆ˜ìµë¥ (%)'] > 0).sum():,}ê°œ ({(results_df['ìµœëŒ€ìˆ˜ìµë¥ (%)'] > 0).mean()*100:.1f}%)")
        
        # íƒ€ì„í”„ë ˆì„ë³„ ìš”ì•½
        print(f"\nğŸ“ˆ íƒ€ì„í”„ë ˆì„ë³„ í‰ê·  ìˆ˜ìµë¥ :")
        tf_summary = results_df.groupby('íƒ€ì„í”„ë ˆì„')['ìµœëŒ€ìˆ˜ìµë¥ (%)'].agg(['count', 'mean']).round(2)
        for tf, row in tf_summary.iterrows():
            print(f"   {tf}: {row['count']:,}ê°œ, í‰ê·  {row['mean']:.2f}%")
            
    else:
        print("âŒ ê²°ê³¼ ì—†ìŒ")

if __name__ == "__main__":
    analyze_performance()