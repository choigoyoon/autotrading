import os
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numba
import numpy as np
import numpy.typing as npt
import pandas as pd
from tqdm import tqdm

# --- íƒ€ì… ì •ì˜ ---
OhlcvData = Dict[str, npt.NDArray[Any]]
AnalysisResult = Dict[str, Any]

# --- ì„¤ì • í´ë˜ìŠ¤ ---
class Config:
    """ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ì˜ ì„¤ì •ì„ ê´€ë¦¬í•©ë‹ˆë‹¤."""
    PROJECT_ROOT: Path = Path(__file__).resolve().parent.parent
    LABELS_DIR: Path = PROJECT_ROOT / 'data' / 'labels_macd' / 'btc_usdt_kst'
    OHLCV_DIR: Path = PROJECT_ROOT / 'data' / 'processed' / 'btc_usdt_kst' / 'resampled_ohlcv'
    RESULTS_DIR: Path = PROJECT_ROOT / 'results' / 'label_performance'
    MERGED_LABELS_FILE: Path = LABELS_DIR / 'btc_usdt_kst_merged_labels.parquet'
    OUTPUT_FILE: Path = RESULTS_DIR / 'performance_analysis_parallel.csv'
    
    # CPU ì½”ì–´ ìˆ˜ì˜ ì ˆë°˜ ë˜ëŠ” ìµœëŒ€ 16ê°œ ì¤‘ ì‘ì€ ê°’ì„ ì‚¬ìš©
    MAX_WORKERS: int = min(os.cpu_count() or 1, 16)
    CHUNK_SIZE: int = 50000

    TF_MAP: Dict[str, str] = {
        '1D': '1day', '2D': '2day', '3D': '3day', '1W': '1week',
        '1h': '1h', '2h': '2h', '4h': '4h', '6h': '6h', '8h': '8h', '12h': '12h',
        '1min': '1min', '3min': '3min', '5min': '5min', '10min': '10min',
        '15min': '15min', '30min': '30min'
    }

# --- Numba ìµœì í™” í•¨ìˆ˜ ---
@numba.njit(fastmath=True)
def analyze_single_signal(
    signal_ts: np.int64, 
    signal_price: np.float64, 
    signal_label: np.int64, 
    ohlcv_timestamps: npt.NDArray[np.int64], 
    ohlcv_high: npt.NDArray[np.float64], 
    ohlcv_low: npt.NDArray[np.float64], 
) -> Tuple[float, int, int, int]:
    """ë‹¨ì¼ ì‹ í˜¸ ë¶„ì„ (numba ìµœì í™”)"""
    start_idx = np.searchsorted(ohlcv_timestamps, signal_ts, side='left')
    
    if start_idx >= len(ohlcv_high):
        return 0.0, -1, 0, -1

    future_high = ohlcv_high[start_idx:]
    future_low = ohlcv_low[start_idx:]
    
    if len(future_high) == 0:
        return 0.0, -1, 0, -1
    
    max_profit_pct: float = 0.0
    peak_idx: int = -1
    
    if signal_label == 1:  # ë§¤ìˆ˜
        peak_idx = int(np.argmax(future_high))
        peak_price = future_high[peak_idx]
        max_profit_pct = ((peak_price - signal_price) / signal_price) * 100
    else:  # ë§¤ë„
        peak_idx = int(np.argmin(future_low))
        trough_price = future_low[peak_idx]
        max_profit_pct = ((signal_price - trough_price) / signal_price) * 100
    
    status: int = 1  # 1: Holding, 2: Breakeven Exit
    exit_idx: int = -1
    
    if peak_idx < len(future_high) - 1:
        after_peak_high = future_high[peak_idx + 1:]
        after_peak_low = future_low[peak_idx + 1:]
        
        if signal_label == 1:
            indices = np.where(after_peak_low <= signal_price)[0]
            if len(indices) > 0:
                status = 2
                exit_idx = peak_idx + 1 + int(indices[0])
        else:
            indices = np.where(after_peak_high >= signal_price)[0]
            if len(indices) > 0:
                status = 2
                exit_idx = peak_idx + 1 + int(indices[0])
    
    final_peak_idx = start_idx + peak_idx if peak_idx != -1 else -1
    final_exit_idx = start_idx + exit_idx if exit_idx != -1 else -1
    return max_profit_pct, int(final_peak_idx), status, int(final_exit_idx)

# --- ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ í•¨ìˆ˜ ---
def process_timeframe_chunk(
    timeframe: str,
    chunk_df: pd.DataFrame,
    ohlcv_data: OhlcvData
) -> Optional[pd.DataFrame]:
    """íƒ€ì„í”„ë ˆì„ ì²­í¬ë¥¼ ì²˜ë¦¬í•˜ê³  ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        signal_timestamps: npt.NDArray[np.int64] = chunk_df.index.view('int64')
        signal_prices: npt.NDArray[np.float64] = chunk_df['close'].to_numpy(dtype=np.float64)
        signal_labels: npt.NDArray[np.int64] = chunk_df['label'].to_numpy(dtype=np.int64)
        
        ohlcv_ts = ohlcv_data['timestamps']
        ohlcv_h = ohlcv_data['high']
        ohlcv_l = ohlcv_data['low']
        
        results: List[AnalysisResult] = []
        
        for i in range(len(signal_timestamps)):
            max_profit, peak_idx, status, exit_idx = analyze_single_signal(
                signal_timestamps[i], signal_prices[i], signal_labels[i],
                ohlcv_ts, ohlcv_h, ohlcv_l
            )
            
            peak_timestamp = ohlcv_ts[peak_idx] if peak_idx != -1 else np.nan
            exit_timestamp = ohlcv_ts[exit_idx] if exit_idx != -1 else np.nan
            
            candles_to_exit = np.nan
            if exit_idx != -1:
                entry_idx = np.searchsorted(ohlcv_ts, signal_timestamps[i], side='left')
                candles_to_exit = exit_idx - entry_idx
            
            results.append({
                'ì§„ì…ì‹œì ': pd.to_datetime(signal_timestamps[i]),
                'íƒ€ì„í”„ë ˆì„': timeframe,
                'ì§„ì…ê°€ê²©': signal_prices[i],
                'ì‹ í˜¸íƒ€ì…': 'Buy' if signal_labels[i] == 1 else 'Sell',
                'ìµœëŒ€ìˆ˜ìµë¥ (%)': max_profit,
                'ìµœê³ (ì €)ì ì‹œì ': pd.to_datetime(peak_timestamp, errors='coerce'),
                'ìµœì¢…ìƒíƒœ': 'Holding' if status == 1 else 'Breakeven Exit',
                'ì²­ì‚°ì‹œì ': pd.to_datetime(exit_timestamp, errors='coerce'),
                'ì²­ì‚°ê¹Œì§€ìº”ë“¤ìˆ˜': float(candles_to_exit),
            })
        
        return pd.DataFrame(results) if results else None
    except Exception as e:
        print(f"[ì˜¤ë¥˜] {timeframe} ì²­í¬ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return None

# --- ë©”ì¸ ë¶„ì„ ë¡œì§ ---
def load_ohlcv_data(config: Config, timeframes: npt.NDArray[Any]) -> Dict[str, OhlcvData]:
    """í•„ìš”í•œ ëª¨ë“  OHLCV ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    ohlcv_data: Dict[str, OhlcvData] = {}
    print("2. OHLCV ë°ì´í„° ë¡œë“œ ì¤‘...")
    for tf in timeframes:
        file_name = config.TF_MAP.get(tf, tf)
        ohlcv_file = config.OHLCV_DIR / f"{file_name}.parquet"
        
        if not ohlcv_file.exists():
            ohlcv_file = config.OHLCV_DIR / f"{tf.replace('min', 'm')}.parquet"
        
        if ohlcv_file.exists():
            try:
                ohlcv_df = pd.read_parquet(ohlcv_file)
                ohlcv_data[tf] = {
                    'timestamps': ohlcv_df.index.view('int64'),
                    'high': ohlcv_df['high'].to_numpy(dtype=np.float64),
                    'low': ohlcv_df['low'].to_numpy(dtype=np.float64)
                }
                print(f"   {tf}: {ohlcv_df.shape}")
            except Exception as e:
                print(f"âš ï¸ {tf} ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            print(f"âš ï¸ {tf} íŒŒì¼ ì—†ìŒ")
    return ohlcv_data

def analyze_performance() -> None:
    """ì„±ê³¼ ë¶„ì„ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    config = Config()
    config.RESULTS_DIR.mkdir(parents=True, exist_ok=True)
    print(f"CPU ì½”ì–´ ìˆ˜: {os.cpu_count()}, ì‚¬ìš©í•  ì›Œì»¤ ìˆ˜: {config.MAX_WORKERS}")

    if not config.MERGED_LABELS_FILE.exists():
        print(f"âŒ ì˜¤ë¥˜: ë³‘í•©ëœ ë¼ë²¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config.MERGED_LABELS_FILE}")
        return

    print("1. ë³‘í•©ëœ ë¼ë²¨ ë°ì´í„° ë¡œë“œ ì¤‘...")
    signals_df = pd.read_parquet(config.MERGED_LABELS_FILE)
    print(f"   ì´ {len(signals_df):,}ê°œì˜ ì‹ í˜¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")

    ohlcv_data = load_ohlcv_data(config, signals_df['timeframe'].unique())

    print("3. ë³‘ë ¬ ì„±ê³¼ ë¶„ì„ ì‹œì‘...")
    tasks: List[Tuple[str, pd.DataFrame, OhlcvData]] = []
    for tf_obj, group in signals_df.groupby(by='timeframe'):
        tf = str(tf_obj)
        if tf in ohlcv_data:
            for i in range(0, len(group), config.CHUNK_SIZE):
                chunk = group.iloc[i:i + config.CHUNK_SIZE]
                tasks.append((tf, chunk, ohlcv_data[tf]))
    
    print(f"   ì´ {len(tasks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
    
    results: List[pd.DataFrame] = []
    with ProcessPoolExecutor(max_workers=config.MAX_WORKERS) as executor:
        future_to_chunk = {
            executor.submit(process_timeframe_chunk, *task): task for task in tasks
        }
        for future in tqdm(as_completed(future_to_chunk), total=len(tasks), desc="ë³‘ë ¬ ë¶„ì„"):
            try:
                result = future.result()
                if result is not None:
                    results.append(result)
            except Exception as e:
                print(f"ì²­í¬ ì²˜ë¦¬ ì¤‘ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ì˜ˆì™¸ ë°œìƒ: {e}")

    if not results:
        print("âŒ ë¶„ì„ ê²°ê³¼ ì—†ìŒ")
        return

    print("4. ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥...")
    results_df = pd.concat(results, ignore_index=True)
    results_df.to_csv(config.OUTPUT_FILE, index=False, encoding='utf-8-sig')

    print(f"\nğŸ‰ ì„±ê³¼ ë¶„ì„ ì™„ë£Œ!")
    print(f"âœ… ê²°ê³¼ ì €ì¥: {config.OUTPUT_FILE}")
    print_summary_statistics(results_df)

def print_summary_statistics(df: pd.DataFrame) -> None:
    """ìµœì¢… ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ ìš”ì•½ í†µê³„ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print(f"   - ì´ ë¶„ì„ëœ ì‹ í˜¸: {len(df):,}ê°œ")
    print(f"   - í‰ê·  ìµœëŒ€ ìˆ˜ìµë¥ : {df['ìµœëŒ€ìˆ˜ìµë¥ (%)'].mean():.2f}%")
    
    print(f"\nğŸ“Š ë¹ ë¥¸ í†µê³„:")
    print(f"   - Holding: {(df['ìµœì¢…ìƒíƒœ'] == 'Holding').sum():,}ê°œ")
    print(f"   - Breakeven Exit: {(df['ìµœì¢…ìƒíƒœ'] == 'Breakeven Exit').sum():,}ê°œ")
    print(f"   - ì–‘ìˆ˜ ìˆ˜ìµë¥ : {(df['ìµœëŒ€ìˆ˜ìµë¥ (%)'] > 0).sum():,}ê°œ ({(df['ìµœëŒ€ìˆ˜ìµë¥ (%)'] > 0).mean()*100:.1f}%)")
    
    print(f"\nğŸ“ˆ íƒ€ì„í”„ë ˆì„ë³„ í‰ê·  ìˆ˜ìµë¥ :")
    tf_summary = df.groupby('íƒ€ì„í”„ë ˆì„')['ìµœëŒ€ìˆ˜ìµë¥ (%)'].agg(['count', 'mean']).round(2)
    with pd.option_context('display.max_rows', None):
        print(tf_summary)

if __name__ == "__main__":
    analyze_performance()