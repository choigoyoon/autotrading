import pandas as pd
from pathlib import Path
from sklearn.preprocessing import MinMaxScaler
from tqdm import tqdm
import torch
import joblib
import warnings
import shutil

warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)

def generate_and_save_sequences(sequence_length: int = 240, test_size: float = 0.1, val_size: float = 0.2):
    """
    MACD ë¼ë²¨ì„ ì‚¬ìš©í•˜ì—¬ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ê³ , ì‹œê³„ì—´ ìˆœì„œë¥¼ ìœ ì§€í•˜ì—¬ ë¶„í•  í›„ ì €ì¥í•©ë‹ˆë‹¤.
    """
    print("ğŸš€ MACD ë¼ë²¨ ê¸°ë°˜ ì‹œí€€ìŠ¤ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # --- 1. í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì • ---
    project_root = Path(__file__).parent.parent
    print(f"ğŸ“ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
    
    # --- 2. ë°ì´í„° ê²½ë¡œ ì„¤ì • ë° í™•ì¸ ---
    labels_dir = project_root / 'data' / 'processed' / 'btc_usdt_kst' / 'labeled'
    ohlcv_path = project_root / 'data' / 'processed' / 'btc_usdt_kst' / 'resampled_ohlcv' / '1min.parquet'
    
    print(f"ğŸ“Š ë¼ë²¨ ë””ë ‰í† ë¦¬: {labels_dir}")
    print(f"ğŸ“ˆ OHLCV íŒŒì¼: {ohlcv_path}")
    
    # --- 3. ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ---
    if not labels_dir.exists():
        print(f"âŒ ë¼ë²¨ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {labels_dir}")
        return False
    
    if not ohlcv_path.exists():
        print(f"âŒ OHLCV íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {ohlcv_path}")
        return False
    
    # --- 4. ë¼ë²¨ íŒŒì¼ ì°¾ê¸° ë° ë¡œë“œ ---
    label_files = list(labels_dir.glob('*.parquet'))
    if not label_files:
        print(f"âŒ ë¼ë²¨ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {labels_dir}")
        return False
    
    print(f"ğŸ“‹ ë°œê²¬ëœ ë¼ë²¨ íŒŒì¼: {len(label_files)}ê°œ")
    
    # 1ë¶„ë´‰ ë¼ë²¨ íŒŒì¼ì„ ìš°ì„  ì°¾ê¸°
    one_min_label_file = None
    for file in label_files:
        if '1min' in file.name.lower() or 'merged' in file.name.lower():
            one_min_label_file = file
            break
    
    if one_min_label_file is None:
        # ê°€ì¥ í° íŒŒì¼ ì„ íƒ
        one_min_label_file = max(label_files, key=lambda f: f.stat().st_size)
    
    print(f"ğŸ¯ ì‚¬ìš©í•  ë¼ë²¨ íŒŒì¼: {one_min_label_file.name}")
    
    # --- 5. ë°ì´í„° ë¡œë“œ ---
    print("ğŸ“Š ë¼ë²¨ ë°ì´í„° ë¡œë“œ ì¤‘...")
    try:
        df_labels = pd.read_parquet(one_min_label_file)
        print(f"âœ… ë¼ë²¨ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_labels):,}í–‰")
        print(f"   ë¼ë²¨ ì»¬ëŸ¼: {df_labels.columns.tolist()}")
        print(f"   ë¼ë²¨ ì¸ë±ìŠ¤ íƒ€ì…: {type(df_labels.index)}")
        print(f"   ë¼ë²¨ ì¸ë±ìŠ¤ ìƒ˜í”Œ: {df_labels.index[:3]}")
    except Exception as e:
        print(f"âŒ ë¼ë²¨ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False
    
    print("ğŸ“ˆ OHLCV ë°ì´í„° ë¡œë“œ ì¤‘...")
    try:
        df_ohlcv = pd.read_parquet(ohlcv_path)
        print(f"âœ… OHLCV ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_ohlcv):,}í–‰")
        print(f"   OHLCV ì»¬ëŸ¼: {df_ohlcv.columns.tolist()}")
        print(f"   OHLCV ì¸ë±ìŠ¤ íƒ€ì…: {type(df_ohlcv.index)}")
        print(f"   OHLCV ì¸ë±ìŠ¤ ìƒ˜í”Œ: {df_ohlcv.index[:3]}")
    except Exception as e:
        print(f"âŒ OHLCV ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False
    
    # --- 6. ì¸ë±ìŠ¤ êµ¬ì¡° ë¶„ì„ ë° ìˆ˜ì • ---
    print("\nğŸ” ì¸ë±ìŠ¤ êµ¬ì¡° ë¶„ì„...")
    
    # ë¼ë²¨ ë°ì´í„°ì— timestamp ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš° ì¸ë±ìŠ¤ë¡œ ì„¤ì •
    if 'timestamp' in df_labels.columns:
        print("ğŸ”„ ë¼ë²¨ ë°ì´í„°ì˜ timestamp ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •...")
        df_labels = df_labels.set_index('timestamp')
        print(f"   ìƒˆ ë¼ë²¨ ì¸ë±ìŠ¤: {df_labels.index[:3]}")
    
    # OHLCV ë°ì´í„° ì¸ë±ìŠ¤ê°€ ë¬¸ìì—´ì¸ ê²½ìš° datetimeìœ¼ë¡œ ë³€í™˜
    if not isinstance(df_ohlcv.index, pd.DatetimeIndex):
        print("ğŸ”„ OHLCV ì¸ë±ìŠ¤ë¥¼ datetimeìœ¼ë¡œ ë³€í™˜...")
        try:
            df_ohlcv.index = pd.to_datetime(df_ohlcv.index)
        except:
            # ì¸ë±ìŠ¤ ë³€í™˜ ì‹¤íŒ¨ì‹œ reset_index í›„ ë‹¤ì‹œ ì‹œë„
            df_ohlcv = df_ohlcv.reset_index()
            if 'timestamp' in df_ohlcv.columns:
                df_ohlcv = df_ohlcv.set_index('timestamp')
            elif 'index' in df_ohlcv.columns:
                df_ohlcv.index = pd.to_datetime(df_ohlcv['index'])
                df_ohlcv = df_ohlcv.drop('index', axis=1)
        print(f"   ìƒˆ OHLCV ì¸ë±ìŠ¤: {df_ohlcv.index[:3]}")
    
    # ë¼ë²¨ ë°ì´í„° ì¸ë±ìŠ¤ë„ datetimeìœ¼ë¡œ ë³€í™˜
    if not isinstance(df_labels.index, pd.DatetimeIndex):
        print("ğŸ”„ ë¼ë²¨ ì¸ë±ìŠ¤ë¥¼ datetimeìœ¼ë¡œ ë³€í™˜...")
        try:
            df_labels.index = pd.to_datetime(df_labels.index)
        except Exception as e:
            print(f"âŒ ë¼ë²¨ ì¸ë±ìŠ¤ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return False
    
    # --- 7. 1ë¶„ë´‰ ë°ì´í„°ë§Œ í•„í„°ë§ (ë¼ë²¨ ë°ì´í„°ê°€ ë‹¤ì¤‘ íƒ€ì„í”„ë ˆì„ì¸ ê²½ìš°) ---
    if 'timeframe' in df_labels.columns:
        print("ğŸ”„ 1ë¶„ë´‰ ë¼ë²¨ë§Œ í•„í„°ë§...")
        one_min_labels = df_labels[df_labels['timeframe'] == '1min'].copy()
        if len(one_min_labels) == 0:
            # 1minì´ ì—†ìœ¼ë©´ ë‹¤ë¥¸ í˜•ì‹ ì‹œë„
            for tf in ['1m', '1Min', '1T']:
                one_min_labels = df_labels[df_labels['timeframe'] == tf].copy()
                if len(one_min_labels) > 0:
                    break
        
        if len(one_min_labels) > 0:
            df_labels = one_min_labels
            print(f"   1ë¶„ë´‰ ë¼ë²¨: {len(df_labels):,}í–‰")
        else:
            print("âš ï¸ 1ë¶„ë´‰ ë¼ë²¨ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì „ì²´ ë°ì´í„° ì‚¬ìš©")
    
    # --- 8. ë¼ë²¨ ì»¬ëŸ¼ í™•ì¸ ---
    label_column = None
    possible_label_cols = ['label', 'labels', 'target', 'signal', 'position']
    
    for col in possible_label_cols:
        if col in df_labels.columns:
            label_column = col
            break
    
    if label_column is None:
        print("âŒ ë¼ë²¨ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    print(f"âœ… ë¼ë²¨ ì»¬ëŸ¼: {label_column}")
    print(f"   ë¼ë²¨ ë¶„í¬: {df_labels[label_column].value_counts().head()}")
    
    # --- 9. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ---
    print("ğŸ”§ í”¼ì²˜ ìƒì„± ì¤‘...")
    df_ohlcv['return'] = df_ohlcv['close'].pct_change().fillna(0)
    df_ohlcv['volatility'] = df_ohlcv['return'].rolling(20).std().fillna(0)
    df_ohlcv['volume_ma'] = df_ohlcv['volume'].rolling(20).mean().fillna(df_ohlcv['volume'])
    
    features_to_use = ['open', 'high', 'low', 'close', 'volume', 'return', 'volatility', 'volume_ma']
    available_features = [col for col in features_to_use if col in df_ohlcv.columns]
    print(f"ğŸ“Š ì‚¬ìš©í•  í”¼ì²˜: {available_features}")
    
    df_features = df_ohlcv[available_features]

    # --- 10. ì¸ë±ìŠ¤ ë§¤ì¹­ ---
    print("ğŸ”„ ë°ì´í„° ì¸ë±ìŠ¤ ë§¤ì¹­ ì¤‘...")
    print(f"   OHLCV ë°ì´í„° ë²”ìœ„: {df_features.index.min()} ~ {df_features.index.max()}")
    print(f"   ë¼ë²¨ ë°ì´í„° ë²”ìœ„: {df_labels.index.min()} ~ {df_labels.index.max()}")
    
    # ê³µí†µ ì¸ë±ìŠ¤ ì°¾ê¸°
    common_index = df_features.index.intersection(df_labels.index)
    print(f"ğŸ“Š ê³µí†µ ë°ì´í„° í¬ì¸íŠ¸: {len(common_index):,}ê°œ")
    
    if len(common_index) == 0:
        print("âŒ ê³µí†µ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤!")
        print("\nğŸ” ë””ë²„ê¹… ì •ë³´:")
        print(f"   OHLCV ì²« 5ê°œ ì¸ë±ìŠ¤: {df_features.index[:5].tolist()}")
        print(f"   ë¼ë²¨ ì²« 5ê°œ ì¸ë±ìŠ¤: {df_labels.index[:5].tolist()}")
        print(f"   OHLCV íƒ€ì„ì¡´: {getattr(df_features.index, 'tz', 'None')}")
        print(f"   ë¼ë²¨ íƒ€ì„ì¡´: {getattr(df_labels.index, 'tz', 'None')}")
        
        # íƒ€ì„ì¡´ ì œê±° í›„ ì¬ì‹œë„
        print("\nğŸ”„ íƒ€ì„ì¡´ ì œê±° í›„ ì¬ì‹œë„...")
        if hasattr(df_features.index, 'tz') and df_features.index.tz is not None:
            df_features.index = df_features.index.tz_localize(None)
        if hasattr(df_labels.index, 'tz') and df_labels.index.tz is not None:
            df_labels.index = df_labels.index.tz_localize(None)
        
        common_index = df_features.index.intersection(df_labels.index)
        print(f"ğŸ“Š íƒ€ì„ì¡´ ì œê±° í›„ ê³µí†µ ë°ì´í„°: {len(common_index):,}ê°œ")
    
    if len(common_index) < sequence_length * 2:
        print(f"âŒ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìµœì†Œ {sequence_length * 2}ê°œ í•„ìš”, í˜„ì¬ {len(common_index)}ê°œ")
        return False
    
    # ê³µí†µ ì¸ë±ìŠ¤ë¡œ ì •ë ¬
    df_features = df_features.loc[common_index].sort_index()
    df_labels = df_labels.loc[common_index].sort_index()
    
    print(f"âœ… ìµœì¢… ë°ì´í„°: {len(common_index):,}ê°œ")
    print(f"   ì‹œê°„ ë²”ìœ„: {common_index.min()} ~ {common_index.max()}")

    # --- 11. ë°ì´í„° ì •ê·œí™” ---
    print("ğŸ“ í”¼ì²˜ ë°ì´í„° ì •ê·œí™” ì¤‘...")
    scaler = MinMaxScaler()
    df_features_scaled_values = scaler.fit_transform(df_features)
    
    # --- 12. ì‹œí€€ìŠ¤ ìƒì„± ---
    print(f"ğŸ”„ ê³¼ê±° {sequence_length}ë¶„ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
    
    feature_array = df_features_scaled_values
    label_array = df_labels[label_column].values
    
    num_sequences = len(feature_array) - sequence_length + 1
    print(f"ğŸ“Š ìƒì„± ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤ ìˆ˜: {num_sequences:,}ê°œ")
    
    if num_sequences <= 0:
        print("âŒ ì‹œí€€ìŠ¤ ìƒì„± ë¶ˆê°€ëŠ¥")
        return False
    
    # --- 13. ë°ì´í„° ë¶„í•  ---
    print("âœ‚ï¸ train/validation/test ë¶„í•  ì¤‘...")
    train_end_idx = int(num_sequences * (1 - test_size - val_size))
    val_end_idx = int(num_sequences * (1 - test_size))

    datasets_info = {
        "train": (0, train_end_idx),
        "val": (train_end_idx, val_end_idx),
        "test": (val_end_idx, num_sequences)
    }
    
    print(f"ğŸ“Š ë°ì´í„° ë¶„í• :")
    for name, (start, end) in datasets_info.items():
        print(f"   {name}: {end-start:,}ê°œ ì‹œí€€ìŠ¤")

    # --- 14. ì‹œí€€ìŠ¤ ì €ì¥ ---
    output_base_dir = project_root / 'data' / 'sequences_macd'
    if output_base_dir.exists():
        print(f"ğŸ—‘ï¸ ê¸°ì¡´ ì‹œí€€ìŠ¤ ë””ë ‰í† ë¦¬ ì‚­ì œ: {output_base_dir}")
        shutil.rmtree(output_base_dir)

    for name, (start, end) in datasets_info.items():
        output_dir = output_base_dir / name
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ’¾ '{name}' ë°ì´í„°ì…‹ ì €ì¥ ì¤‘...")
        for i in tqdm(range(start, end), desc=f"'{name}' ì €ì¥"):
            sequence = feature_array[i : i + sequence_length]
            label = label_array[i + sequence_length - 1]
            
            feature_tensor = torch.tensor(sequence, dtype=torch.float32)
            label_tensor = torch.tensor(label, dtype=torch.long)
            
            save_idx = i - start
            torch.save({
                'features': feature_tensor,
                'label': label_tensor,
                'timestamp': common_index[i + sequence_length - 1]
            }, output_dir / f"{save_idx}.pt")

    # --- 15. ë©”íƒ€ë°ì´í„° ì €ì¥ ---
    metadata = {
        'sequence_length': sequence_length,
        'num_features': len(available_features),
        'feature_names': available_features,
        'label_column': label_column,
        'num_sequences': {name: end-start for name, (start, end) in datasets_info.items()},
        'data_range': {
            'start': str(common_index[0]),
            'end': str(common_index[-1])
        },
        'label_distribution': df_labels[label_column].value_counts().to_dict()
    }
    
    metadata_path = output_base_dir / "metadata.json"
    import json
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2, default=str)
    
    # --- 16. Scaler ì €ì¥ ---
    scaler_path = output_base_dir / "scaler.joblib"
    joblib.dump(scaler, scaler_path)
    
    print(f"\nâœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_base_dir}")
    print(f"ğŸ“Š ë©”íƒ€ë°ì´í„°: {metadata_path}")
    print(f"âš–ï¸ ì •ê·œí™” Scaler: {scaler_path}")
    
    return True

if __name__ == '__main__':
    success = generate_and_save_sequences()
    if success:
        print("\nğŸ‰ ì‹œí€€ìŠ¤ ìƒì„± ì„±ê³µ!")
    else:
        print("\nâŒ ì‹œí€€ìŠ¤ ìƒì„± ì‹¤íŒ¨!")